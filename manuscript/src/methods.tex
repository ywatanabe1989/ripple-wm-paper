\section{Methods}
\subsection{Dataset}
The dataset used in this study, which is publicly available, comprises nine epilepsy patients performing a modified the Sternberg task \cite{boran_dataset_2020}. This task includes four phases: fixation (1s), encoding (2s), maintenance (3s), and retrieval (2s). During the encoding phase, participants were presented with a set of four, six, or eight alphabet letters. They were then tasked with determining whether a probe letter displayed during the retrieval phase had previously appeared (the correct response for Match IN task) or not (the correct response for Mismatch OUT task). Intracranial electroencephalography (iEEG) signals were captured with a 32 kHz sampling rate within a 0.5--5,000 Hz frequency range, using depth electrodes in the MTL regions: the anterior head of the left and right hippocampus (AHL and AHR), the posterior body of the hippocampus (PHL and PHR), the entorhinal cortex (ECL and ECR), and the amygdala (AL and AR), as depicted in Figure~\ref{fig:01}A and Table~\ref{tab:01}. The iEEG signals were subsequently downsampled to 2 kHz. Correlations between variables such as set size and correct rate were examined (Figure~\ref{fig:s01}S1). Multiunit spike timing was determined via a spike sorting algorithm \cite{niediek_reliable_2016} using the Combinato package (\url{https://github.com/jniediek/combinato})(Figure~\ref{fig:01}C).

\subsection{Calculation of NT using GPFA}
NTs, also referred to as 'factors', in the hippocampus, EC, and amygdala were determined using GPFA \cite{yu_gaussian-process_2009} applied to the multi-unit activity data for each session, performed with the elephant package (\url{https://elephant.readthedocs.io/en/latest/reference/gpfa.html}). The bin size was set to 50 ms, without overlaps. Each factor was z-normalized across all sessions, and the Euclidean distance from the origin ($O$) was then computed.
\\
\indent
An optimal GPFA dimensionality was found to be three using the elbow method obtained by examining the log-likelihood values through a three-fold cross-validation approach (Figure~\ref{fig:02}B).

% \subsection{Calculation of Geometric Medians of NT}
% For each NT within an ROI, the geometric median of the middle of 1 second of each phase was calculated and assigned as $\mathrm{g_{F}}$ for fixation, $\mathrm{g_{E}}$ for encoding, $\mathrm{g_{M}}$ for maintenance, and $\mathrm{g_{R}}$ for retrieval phase. The calculation of geometric medians were processed using the Python geom_median package (\url{https://github.com/krishnap25/geom_median?tab=readme-ov-file}). \cite{pillutla:etal:rfa} Namely, 
% \text{The geometric median of } x_1, \dots, x_n \in \mathbb{R}^d \text{ with weights } \alpha_1, \dots, \alpha_n \geq 0 \text{ is defined as}
% x_{GM} = \arg\min_{z \in \mathbb{R}^d} \sum_{i=1}^n \alpha_i \|z - x_i\|_2.
\subsection{Calculation of Geometric Medians of NT}
For each NT within an ROI, the geometric median (or also known the Fermat point, Weber's L1 median, Fr√©chet median, and so on) of the middle 1 second of each phase was calculated and assigned as $\mathrm{g_{F}}$ for the fixation phase, $\mathrm{g_{E}}$ for the encoding phase, $\mathrm{g_{M}}$ for the maintenance phase, and $\mathrm{g_{R}}$ for the retrieval phase. The calculation of geometric medians was performed using the Python geom_median package (\url{https://github.com/krishnap25/geom_median?tab=readme-ov-file}). \cite{pillutla:etal:rfa} Specifically, 
the geometric median of $x_1, \dots, x_n \in \mathbb{R}^d$ is defined as
$x_{GM} = \arg\min_{z \in \mathbb{R}^d} \sum_{i=1}^n \|z - x_i\|_2$.
\\
\indent
\subsection{Topological Data Analysis}
Geometric medians were considered as the centroid of each class of samples. From these centroids, radii were incrementally increased, and the number of "holes" was counted in response to the radius. This process generates barcode plots to evaluate the stability of each state and persistent homology.
\\
\indent
\subsection{Classification of NT Phases Using a Linear SVM Classifier}
A linear support vector machine classifier was applied to NT data from each session to classify the four phases of the task: Fixation, Encoding, Maintenance, and Retrieval, in a supervised manner. Evaluation was conducted using the three-fold cross-validation method. The classification tasks were also conducted under specific conditions---set size (4, 6, or 8) and task type (Match IN and Mismatch OUT)---as well as an entire session with 50 trials.
\\
\indent
\subsection{Identifying SWR candidates from hippocampal regions}
Potential SWR events within the hippocampus were detected using a widely used method \cite{liu_consensus_2022}. LFP signals from a region of interest (ROI) like AHL, were re-referenced by deducting the averaged signal from locations outside the ROI (for instance, AHR, PHL, PHR, ECL, ECR, AL, and AR). The re-referenced LFP signals were then filtered with a ripple-band filter (80--140 Hz) to determine SWR candidates, marked as $\textrm{SWR}^+$ candidates. SWR detection was carried out using a published tool (\url{https://github.com/Eden-Kramer-Lab/ripple_detection}) \cite{kay_hippocampal_2016}, with the bandpass range adjusted to 80--140 Hz for humans \cite{norman_hippocampal_2019, norman_hippocampal_2021, liu_consensus_2022}, unlike the original 150--250 Hz range typically applied to rodents \cite{foster_reverse_2006, karlsson_awake_2009, carr_hippocampal_2011, pfeiffer_hippocampal_2013, jadhav_awake_2012, singer_hippocampal_2013, buzsaki_hippocampal_2015, wu_hippocampal_2017, fernandez-ruiz_long-duration_2019}.
\\
\indent
Control events for $\textrm{SWR}^+$ candidates, labeled as $\textrm{SWR}^-$ candidates, were detected by randomly shuffling the timestamps of $\textrm{SWR}^+$ candidates across all trials and subjects. The resulting $\textrm{SWR}^+/\textrm{SWR}^-$ candidates were then visually inspected.
\\
\indent
\subsection{Defining SWRs from Putative Hippocampal CA1 Regions Using UMAP Clustering}
Potential SWRs were differentiated from SWR candidates in putative CA1 (cornu Ammonis 1) regions. The definition of putative CA1 regions was as follows. First, $\textrm{SWR}^+$ and $\textrm{SWR}^-$ candidates in the hippocampus were projected into a two-dimensional space using a supervised clustering method, Uniform Manifold Approximation and Projection (UMAP) \cite{mcinnes_umap_2018}. The input features for this projection were the spike counts per unit during the period of $\textrm{SWR}^+$ or $\textrm{SWR}^-$ candidates. Clustering validation was performed by calculating the silhouette score \cite{rousseeuw_silhouettes_1987} from clustered sample points in the corresponding two-dimensional space. Regions in the hippocampus that scored above 0.6 on average across sessions ($75^{th}$ percentile) were identified as putative CA1 regions. This process resulted in the identification of five electrode positions from five patients.
\\
\indent
$\textrm{SWR}^+/\textrm{SWR}^-$ candidates in these predetermined CA1 regions were categorized as $\textrm{SWR}^+/\textrm{SWR}^-$, and thus they no longer retained their candidate status. The duration and ripple band peak amplitude of SWRs were found to follow log-normal distributions. Each time period of SWR was partitioned relative to the time from the SWR center into pre- (at $-800$ ms to $-300$ ms from the SWR center), mid- (at $-250$ to $+250$ ms), and post-SWR (at $+300$ to $+800$ ms) times.
\\
\indent
\subsection{Statistical Evaluation}
Both the Brunner--Munzel test and the Kruskal-Wallis test were executed using the SciPy package in Python \cite{virtanen_scipy_2020}. Correlational analysis was conducted by determining the rank of the observed correlation coefficient within its associated set-size-shuffled surrogate using a customized Python script. The bootstrap test was implemented with an in-house Python script.
\label{sec:methods}